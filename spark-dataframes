1.Create DataFrame from CSV / text 

val df = spark.read.option("header", "false") .option("inferSchema", "true").csv("/data10")

df.show()  
df.printSchema() 


Custom column names 

val df = spark.read.option("header", "false") .option("inferSchema", "true").csv("/data10").toDF("sno", "name", "drug", "gender", "amount")


2. Read a json file ( df) 

val df = spark.read.json("/data.json")
df.show()
df.printSchema()

Input file 

{"id":1, "name":"Alice", "age":30, "salary":50000}
{"id":2, "name":"Bob", "age":25, "salary":45000}
{"id":3, "name":"Charlie", "age":35, "salary":60000}


3.Creating a DataFrame from a Manual List

import spark.implicits._

val data = Seq(
  (1, "gowtham", 30, 50000),
  (2, "kumar", 25, 45000),
  (3, "rahul", 35, 60000)
)

val df = data.toDF("id", "name", "age", "salary")
df.show()

4. Creating a DataFrame from an RDD

val rdd = sc.parallelize(Seq(
  (1, "Alice", 30, 50000),
  (2, "Bob", 25, 45000),
  (3, "Charlie", 35, 60000)
))

import spark.implicits._
val df = rdd.toDF("id", "name", "age", "salary")
df.show()


5.Creating a DataFrame with a Custom Schema

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val schema = StructType(Array(
  StructField("id", IntegerType, true),
  StructField("name", StringType, true),
  StructField("age", IntegerType, true),
  StructField("salary", IntegerType, true)
))

val data = Seq(
  Row(1, "Alice", 30, 50000),
  Row(2, "Bob", 25, 45000),
  Row(3, "Charlie", 35, 60000)
)

val rdd = sc.parallelize(data)
val df = spark.createDataFrame(rdd, schema)

df.show()
df.printSchema()

6. Read from hive 
--hive
create table patient(pid INT, pname STRING, drug STRING,gender STRING, 
tot_amt INT) row format delimited fields terminated by ',' stored as textfile; 


load data local inpath '/home/hadoop/datagen_10.txt'  into table patient; 

--spark-shell
val df = spark.sql("SELECT * FROM my_database.my_table")
df.show()

7. Dataframe select 

val df = spark.read.option("header", "false") .option("inferSchema", "true").csv("/data10").toDF("sno", "name", "drug", "gender", "amount")


val selectdf = df.select("sno","name","drug")
selectdf.show

8. selectExpr 

val df = spark.read.option("header", "false") .option("inferSchema", "true").csv("/data10").toDF("sno", "name", "drug", "gender", "amount")

val selectedDF = df.selectExpr( "sno","name", "drug", "amount * 0.1 as tax_amount" )

selectedDF.show()

9. hive spark joins 

--Hive 
CREATE TABLE IF NOT EXISTS medical_records (sno INT,doctor_name STRING )stored as textile; 

INSERT INTO medical_records VALUES
(1, 'Dr. Kumar'),
(2, 'Dr. Priya'),
(3, 'Dr. Raj'),
(4, 'Dr. Kumar'),
(5, 'Dr. Mehta'),
(6, 'Dr. Sharma'),
(7, 'Dr. Ramesh'),
(8, 'Dr. Raj'),
(9, 'Dr. Mehta'),
(10, 'Dr. Kumar');


---or file 

1,Dr. Kumar
2,Dr. Priya
3,Dr. Raj
4,Dr. Kumar
5,Dr. Mehta
6,Dr. Sharma
7,Dr. Ramesh
8,Dr. Raj
9,Dr. Mehta
10,Dr. Kumar
------

select  a.pname, b.doctor_name from patient a left join medical_records b on a.pid = b.sno;


-- spark with hive join via df functions 

val patientdf = spark.read.table("patient")
val dcotordf = spark.read.table("medical_records")

patientdf.show 
dcotordf.show()


import org.apache.spark.sql.functions._
val joindf = patientdf.join (dcotordf , patientdf("pid") === dcotordf("sno"), "left")
joindf.printSchema
joindf.select ("pname","doctor_name").show



10. withcolumn transformation 

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

import spark.implicits._

val data = Seq(
  (1, "Gowtham", "Avil", "Male", 200),
  (2, "Priya", "Metacin", "Female", 50),
  (3, "Rahul", "Paracetamol", "Male", 120),
  (4, "Divya", "Avil", "Female", 300),
  (5, "Arjun", "Metacin", "Male", 80)
)

val df = data.toDF("sno", "name", "drug", "gender", "amount")

val renamedDF = df.withColumnRenamed("sno", "transaction_id").withColumnRenamed("name", "customer_name").withColumnRenamed("drug", "medicine").withColumnRenamed("amount", "total_amount")

val transformedDF = renamedDF.withColumn("tax_amount", col("total_amount") * 0.1)

val filteredDF = transformedDF.filter($"total_amount" > 100)

filteredDF.show()

filteredDF.rdd.map(_.mkString(",")).saveAsTextFile("/load_tax_data")


